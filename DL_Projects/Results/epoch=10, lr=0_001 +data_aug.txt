================== epoch 1 ==================
1: train_loss=1.1568006771272108
2: train_loss=1.4245325969512856
3: train_loss=0.8196125664714959
4: train_loss=0.7414515012813846
5: train_loss=0.6553677274743904
6: train_loss=0.7204848944095682
7: train_loss=0.6757493975749022
8: train_loss=0.6548635019510145
9: train_loss=0.6833601911218848
10: train_loss=0.6190285078561285
11: train_loss=0.6473608017320486
12: train_loss=0.6530553786357587
13: train_loss=0.5803682889620492
14: train_loss=0.6240255014562847
15: train_loss=0.6359508278670026
16: train_loss=0.7323202754034926
17: train_loss=0.634150528043642
18: train_loss=0.567632396341075
19: train_loss=0.5783033417932456
20: train_loss=0.597814990146413
21: train_loss=0.5217651000821292
22: train_loss=0.4868622790279703
23: train_loss=0.5185375928526255
24: train_loss=0.6104165261383533
25: train_loss=0.2918206350946851
26: train_loss=0.48967692757518333
27: train_loss=0.5381286517619996
28: train_loss=0.5872172954695711
29: train_loss=0.31759218730991745
30: train_loss=0.6077325605978449

evaluation 1: train_loss=0.6457327882836854, test_loss=0.5614853133922927, test_acc=0.7333333333333333

================== epoch 2 ==================
1: train_loss=0.5748607411132001
2: train_loss=0.43380124344874477
3: train_loss=0.4692529849736246
4: train_loss=0.35542436410875383
5: train_loss=0.7182541703882634
6: train_loss=0.5520441905052033
7: train_loss=0.6791702421485101
8: train_loss=0.6395636543410519
9: train_loss=0.4833520329320805
10: train_loss=0.4190345994238246
11: train_loss=0.53843330256318
12: train_loss=0.5138339955453252
13: train_loss=0.6572013092157263
14: train_loss=0.5125529276289308
15: train_loss=0.458753690689474
16: train_loss=0.393573745750734
17: train_loss=0.5776935812360855
18: train_loss=0.492245338646859
19: train_loss=0.4936804109267009
20: train_loss=0.43260403307415746
21: train_loss=0.45046737914650775
22: train_loss=0.6207356434179196
23: train_loss=0.531459475058188
24: train_loss=0.44282760966327533
25: train_loss=0.5355599660068818
26: train_loss=0.4908946124311932
27: train_loss=0.4646973507856395
28: train_loss=0.48014298842471176
29: train_loss=0.40853334772883676
30: train_loss=0.36281061932632586

evaluation 2: train_loss=0.5061153183549969, test_loss=0.4087455404325153, test_acc=0.9333333333333333

================== epoch 3 ==================
1: train_loss=0.3781330659457928
2: train_loss=0.32248623639582996
3: train_loss=0.31410341046316664
4: train_loss=0.6448323190722893
5: train_loss=0.7385141354599238
6: train_loss=0.652921795460077
7: train_loss=0.1701666975143083
8: train_loss=0.5032614338717553
9: train_loss=0.32666933487133226
10: train_loss=0.34010128753773305
11: train_loss=0.38436870546631163
12: train_loss=0.37608202101461047
13: train_loss=0.30874732277583516
14: train_loss=0.41853789301441663
15: train_loss=0.3954188988391917
16: train_loss=0.41477329263940316
17: train_loss=0.2987105226041467
18: train_loss=0.18615038727678185
19: train_loss=0.3879690690014802
20: train_loss=0.3678327202822304
21: train_loss=0.4137240340396533
22: train_loss=0.45870677797644627
23: train_loss=0.3419068345360356
24: train_loss=0.46500063518881607
25: train_loss=0.3103710122479806
26: train_loss=0.5615974483117522
27: train_loss=0.4757354074235831
28: train_loss=0.44356547698313764
29: train_loss=0.3943658884777114
30: train_loss=0.42188602292085

evaluation 3: train_loss=0.40722133625375273, test_loss=0.35655709702690286, test_acc=0.8666666666666667

================== epoch 4 ==================
1: train_loss=0.5613682858003266
2: train_loss=0.5745385338556196
3: train_loss=0.25537956846507986
4: train_loss=0.36661024800454267
5: train_loss=0.44630537757145267
6: train_loss=0.2575444569948232
7: train_loss=0.4275913446969113
8: train_loss=0.32766886703251596
9: train_loss=0.31693052098994784
10: train_loss=0.37969346469815174
11: train_loss=0.2778876188900027
12: train_loss=0.44026956050515786
13: train_loss=0.4399591575777316
14: train_loss=0.253758499592865
15: train_loss=0.35356657987101714
16: train_loss=0.17621080222540542
17: train_loss=0.5768433251625414
18: train_loss=0.3183370512346711
19: train_loss=0.19371297613955507
20: train_loss=0.4155805859233207
21: train_loss=0.3925426456009885
22: train_loss=0.3655089006052799
23: train_loss=0.4449785565948988
24: train_loss=0.5416673168938275
25: train_loss=0.31222293031671394
26: train_loss=0.3433554848408806
27: train_loss=0.18715357334461355
28: train_loss=0.40047323663539725
29: train_loss=0.669745643874242
30: train_loss=0.6726717629209275

evaluation 4: train_loss=0.38966922922864694, test_loss=0.3099881885663787, test_acc=0.9

================== epoch 5 ==================
1: train_loss=0.25345471723859675
2: train_loss=0.2728091049791673
3: train_loss=0.29155094748638466
4: train_loss=0.30070997164178337
5: train_loss=0.2820260836684028
6: train_loss=0.34757832211199907
7: train_loss=0.31815029796788213
8: train_loss=0.3185397604293886
9: train_loss=0.3143239617933542
10: train_loss=0.43228057523439073
11: train_loss=0.27234394290109987
12: train_loss=0.2755075686342802
13: train_loss=0.45395373018903035
14: train_loss=0.49929403002818246
15: train_loss=0.40021796008675087
16: train_loss=0.41113414206034643
17: train_loss=0.26600988099294876
18: train_loss=0.3130177116699416
19: train_loss=0.4092710324453789
20: train_loss=0.4757900940039009
21: train_loss=0.329679768993913
22: train_loss=0.45032895094964437
23: train_loss=0.28547539770013175
24: train_loss=0.328505312876681
25: train_loss=0.35643683643741947
26: train_loss=0.3619784211226085
27: train_loss=0.589553899525868
28: train_loss=0.7824871709687348
29: train_loss=0.5591376621719026
30: train_loss=0.4945121685501722

evaluation 5: train_loss=0.3815353141620095, test_loss=0.4460030983290139, test_acc=0.9333333333333333

================== epoch 6 ==================
1: train_loss=0.3659255201947788
2: train_loss=0.2837638938319421
3: train_loss=0.4628783113605689
4: train_loss=0.46485847673397335
5: train_loss=0.4193222157165842
6: train_loss=0.42448094551487936
7: train_loss=0.4754224934824384
8: train_loss=0.37138407419962843
9: train_loss=0.2999150144644078
10: train_loss=0.3230665497898728
11: train_loss=0.2823617298443362
12: train_loss=0.344146414102307
13: train_loss=0.29802500844931523
14: train_loss=0.4476860738644929
15: train_loss=0.38273357190658513
16: train_loss=0.4247892051397699
17: train_loss=0.3390873042700966
18: train_loss=0.324955542076078
19: train_loss=0.4015593691664379
20: train_loss=0.6571563814548804
21: train_loss=0.44886734437066483
22: train_loss=0.5297974075472031
23: train_loss=0.3400049338647192
24: train_loss=0.32685879361786857
25: train_loss=0.33131915128934747
26: train_loss=0.3599951603763726
27: train_loss=0.39041221618922956
28: train_loss=0.3237164658487756
29: train_loss=0.45830419953549373
30: train_loss=0.4891257787659283

evaluation 6: train_loss=0.3930639848989659, test_loss=0.4238994106438077, test_acc=0.9333333333333333

================== epoch 7 ==================
1: train_loss=0.23068213131003645
2: train_loss=0.35550735734180355
3: train_loss=0.2683757023494022
4: train_loss=0.3209972537322426
5: train_loss=0.25599979637643633
6: train_loss=0.5688895229524636
7: train_loss=0.300372656340058
8: train_loss=0.21748043709814222
9: train_loss=0.5009476536818128
10: train_loss=0.2678526716640638
11: train_loss=0.47319071368617466
12: train_loss=0.11787127628646264
13: train_loss=0.4591071301485902
14: train_loss=0.37655661927831924
15: train_loss=0.38299769849614484
16: train_loss=0.447222628176478
17: train_loss=0.3925947830313162
18: train_loss=0.269673887158744
19: train_loss=0.414550994749423
20: train_loss=0.3525978547167261
21: train_loss=0.2814793262328158
22: train_loss=0.42073807672168845
23: train_loss=0.34488338765690163
24: train_loss=0.5047154537697457
25: train_loss=0.4423716880525267
26: train_loss=0.3521732211742815
27: train_loss=0.4571172960322278
28: train_loss=0.43622979081121926
29: train_loss=0.9839379376789891
30: train_loss=0.4468428740100785

evaluation 7: train_loss=0.3881319273571772, test_loss=0.535313463997115, test_acc=0.7333333333333333

================== epoch 8 ==================
1: train_loss=0.5640347495009219
2: train_loss=0.5507320331266613
3: train_loss=0.4639126973298514
4: train_loss=0.46079607441345033
5: train_loss=0.3971492797912244
6: train_loss=0.4565023521672449
7: train_loss=0.42418438125840935
8: train_loss=0.3913379689567787
9: train_loss=0.42615036920949917
10: train_loss=0.3241155403922675
11: train_loss=0.2649477098850212
12: train_loss=0.3423737579238961
13: train_loss=0.560674657631824
14: train_loss=0.28950157167943563
15: train_loss=0.19687817841921915
16: train_loss=0.3579355677509681
17: train_loss=0.2274912813301887
18: train_loss=0.2814116706074692
19: train_loss=0.31973783191553584
20: train_loss=0.18939966732317
21: train_loss=0.5033334590977796
22: train_loss=0.25580755295000246
23: train_loss=0.449670279774048
24: train_loss=0.4839836398697043
25: train_loss=0.28195809837530145
26: train_loss=0.3789692688360695
27: train_loss=0.7873311130284903
28: train_loss=0.5737504938105568
29: train_loss=0.5840187104816509
30: train_loss=0.5527942925324038

evaluation 8: train_loss=0.4113628083123016, test_loss=0.5300896419127137, test_acc=0.9

================== epoch 9 ==================
1: train_loss=0.28594081836897184
2: train_loss=0.4654026075524615
3: train_loss=0.5651816960029047
4: train_loss=0.5530342978297621
5: train_loss=0.4827169129865395
6: train_loss=0.4937018867417779
7: train_loss=0.4933865317186677
8: train_loss=0.5396160183135577
9: train_loss=0.41780809374224337
10: train_loss=0.49122138423181233
11: train_loss=0.4906717506622775
12: train_loss=0.41475968638193933
13: train_loss=0.47425739919132925
14: train_loss=0.5025693546904599
15: train_loss=0.47496401858665455
16: train_loss=0.36346516059533257
17: train_loss=0.26897274970175666
18: train_loss=0.26122777074871284
19: train_loss=0.5093809429058406
20: train_loss=0.3696649367563791
21: train_loss=0.6889926306313262
22: train_loss=0.3163890190144069
23: train_loss=0.3420650414562472
24: train_loss=0.2617214329917756
25: train_loss=0.29548177836247114
26: train_loss=0.24422686588108533
27: train_loss=0.20830013646951198
28: train_loss=0.4263988994959737
29: train_loss=0.23879733921020782
30: train_loss=0.491593913786675

evaluation 9: train_loss=0.41439703583363546, test_loss=0.4121945377190835, test_acc=0.9666666666666667

================== epoch 10 ==================
1: train_loss=0.2997260131934159
2: train_loss=0.28637328763464087
3: train_loss=0.4180757475350857
4: train_loss=0.20920978531692336
5: train_loss=0.20759491062915636
6: train_loss=0.33535269313467625
7: train_loss=0.4029620254866895
8: train_loss=0.41044762338706164
9: train_loss=0.2821718305398756
10: train_loss=0.34505362249490884
11: train_loss=0.1584887793259708
12: train_loss=0.5407450241851814
13: train_loss=0.37122643561351143
14: train_loss=0.2567246789169884
15: train_loss=0.2965769749357952
16: train_loss=0.3719480707950447
17: train_loss=0.3416255029165201
18: train_loss=0.473823361456669
19: train_loss=0.39072331414615713
20: train_loss=0.5067966016983242
21: train_loss=0.3630949353501707
22: train_loss=0.31751048679542165
23: train_loss=0.39153302715947663
24: train_loss=0.507693267804847
25: train_loss=0.35417146842324976
26: train_loss=0.43267830068581864
27: train_loss=0.40077987415218097
28: train_loss=0.4477149160941713
29: train_loss=0.2801495987158695
30: train_loss=0.29045230828431445

evaluation 10: train_loss=0.35638081556027057, test_loss=0.2624159227096168, test_acc=0.9

================== Final Test Accuracy ==================
test_acc:0.96
Saved Network Parameters!
elapsed time : 3h 49m 25s
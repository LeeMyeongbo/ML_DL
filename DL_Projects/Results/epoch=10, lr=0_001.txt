
================== epoch 1 ==================
1: train_loss=2.0049126624490197
2: train_loss=1.155129336174141
3: train_loss=1.0319620625268917
4: train_loss=0.7011441712000868
5: train_loss=0.7170130097194292
6: train_loss=0.7169265029243556
7: train_loss=0.690011385303485
8: train_loss=0.6875903877395856
9: train_loss=0.6955802211834544
10: train_loss=0.7808321177849197
11: train_loss=0.7160080193892366
12: train_loss=0.6783470275429135
13: train_loss=0.6933948379728027

evaluation 1: train_loss=0.8668347493777172, test_loss=0.6909308217043537, test_acc=0.5333333333333333

================== epoch 2 ==================
1: train_loss=0.690334935036559
2: train_loss=0.6935114958028584
3: train_loss=0.6926997817509658
4: train_loss=0.6902882036215754
5: train_loss=0.6922421022182481
6: train_loss=0.6927134213807249
7: train_loss=0.6893397700776852
8: train_loss=0.6938192831986593
9: train_loss=0.6947679110761791
10: train_loss=0.6932099184405163
11: train_loss=0.6933399625308737
12: train_loss=0.6920394744100192
13: train_loss=0.6889677459947956

evaluation 2: train_loss=0.6920980004261278, test_loss=0.6880522213210991, test_acc=0.4

================== epoch 3 ==================
1: train_loss=0.6815571573577847
2: train_loss=0.6479802700363997
3: train_loss=0.6707586726223423
4: train_loss=0.710620252582209
5: train_loss=0.6604121016836212
6: train_loss=0.6621539996763185
7: train_loss=0.6726020658446439
8: train_loss=0.6873106711012436
9: train_loss=0.6870818847248751
10: train_loss=0.6508705070552236
11: train_loss=0.6813884236450541
12: train_loss=0.6826262722793852
13: train_loss=0.6675464074907337

evaluation 3: train_loss=0.6740698989307566, test_loss=0.6981384099854004, test_acc=0.4

================== epoch 4 ==================
1: train_loss=0.5996362682350324
2: train_loss=0.6785519754278072
3: train_loss=0.6618596645250252
4: train_loss=0.7266288637009464
5: train_loss=0.6182160875380986
6: train_loss=0.665516923802199
7: train_loss=0.6614840589115463
8: train_loss=0.6582316629297835
9: train_loss=0.6389846648372929
10: train_loss=0.6291626444418426
11: train_loss=0.67035771968811
12: train_loss=0.6700640373811542
13: train_loss=0.6861402739003052

evaluation 4: train_loss=0.6588334496399342, test_loss=0.7025442187988031, test_acc=0.4666666666666667

================== epoch 5 ==================
1: train_loss=0.6472589253495897
2: train_loss=0.6064578832588827
3: train_loss=0.619374242581627
4: train_loss=0.6650542272457302
5: train_loss=0.6146414943518074
6: train_loss=0.6399159000967038
7: train_loss=0.673833999988834
8: train_loss=0.6482424283685105
9: train_loss=0.6828875978494532
10: train_loss=0.692975677290567
11: train_loss=0.6445534180834248
12: train_loss=0.6747215788643164
13: train_loss=0.6523277208029682

evaluation 5: train_loss=0.6509419303178781, test_loss=0.6643278055216465, test_acc=0.5333333333333333

================== epoch 6 ==================
1: train_loss=0.6640929555598453
2: train_loss=0.6790623645006403
3: train_loss=0.6208509071616645
4: train_loss=0.6636210339265182
5: train_loss=0.5990246150941604
6: train_loss=0.6096848478330118
7: train_loss=0.5831359024846909
8: train_loss=0.5622211444006984
9: train_loss=0.5941132742694732
10: train_loss=0.5255685256257128
11: train_loss=0.5157912208413677
12: train_loss=0.5965413841839847
13: train_loss=0.4901798807148123

evaluation 6: train_loss=0.5926067735843523, test_loss=0.504344051060571, test_acc=0.9333333333333333

================== epoch 7 ==================
1: train_loss=0.48935041209754854
2: train_loss=0.5917351020410546
3: train_loss=0.42982122608256074
4: train_loss=0.49020993771975296
5: train_loss=0.5434875998137463
6: train_loss=0.4731519187665997
7: train_loss=0.45977602598254536
8: train_loss=0.46295424604735375
9: train_loss=0.48543422525212804
10: train_loss=0.4784795000525616
11: train_loss=0.4926065179052929
12: train_loss=0.4185619400176952
13: train_loss=0.3293183677973168

evaluation 7: train_loss=0.4726836168904736, test_loss=0.609591646371926, test_acc=0.7666666666666667

================== epoch 8 ==================
1: train_loss=0.436336783231976
2: train_loss=0.4356744411335893
3: train_loss=0.5162626309779136
4: train_loss=0.3840325370112132
5: train_loss=0.37527674803288985
6: train_loss=0.7398643093735418
7: train_loss=0.5747088229180732
8: train_loss=0.4033490100366288
9: train_loss=0.4905914364817929
10: train_loss=0.5876129653312027
11: train_loss=0.516042488400402
12: train_loss=0.3900700904008956
13: train_loss=0.4165396816830547

evaluation 8: train_loss=0.4820278419240902, test_loss=0.30319308938385187, test_acc=0.8333333333333334

================== epoch 9 ==================
1: train_loss=0.6052909768339665
2: train_loss=0.4000475387548986
3: train_loss=0.26045626185071935
4: train_loss=0.49597919686622516
5: train_loss=0.34870007203063724
6: train_loss=0.5454476602657181
7: train_loss=0.33525250276499563
8: train_loss=0.4662009340262326
9: train_loss=0.48195152778411643
10: train_loss=0.3511255939846793
11: train_loss=0.3343619763212947
12: train_loss=0.30595362695584544
13: train_loss=0.4041288087874231

evaluation 9: train_loss=0.4103766674789808, test_loss=0.5387728668817549, test_acc=0.8666666666666667

================== epoch 10 ==================
1: train_loss=0.3575439398384055
2: train_loss=0.5086479055504592
3: train_loss=0.51786591929105
4: train_loss=0.6350073320948846
5: train_loss=0.4191566267348604
6: train_loss=0.36357203435916197
7: train_loss=0.5158257185855082
8: train_loss=0.3970974681499958
9: train_loss=0.4213739789802044
10: train_loss=0.308006418821548
11: train_loss=0.3645035501412823
12: train_loss=0.29837682111650615
13: train_loss=0.37436620374709517

evaluation 10: train_loss=0.4216418398008432, test_loss=0.4333707981125937, test_acc=0.9333333333333333

================== Final Test Accuracy ==================
test_acc:0.94
Saved Network Parameters!
elapsed time : 3h 16m 14s